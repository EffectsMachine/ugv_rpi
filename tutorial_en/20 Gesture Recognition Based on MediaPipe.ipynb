{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6282ee6c-0e57-40da-806c-86dfa039ea1f",
   "metadata": {},
   "source": [
    "# Gesture Recognition Based on MediaPipe\n",
    "\n",
    "This section introduces how to implement gesture recognition using MediaPipe + OpenCV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3205ce-5a8f-4d79-b4b1-a5a652d1646b",
   "metadata": {},
   "source": [
    "## What is MediaPipe?\r\n",
    "\n",
    "MediaPipe is an open-source framework developed by Google for building machine learning-based multimedia processing applications. It provides a set of tools and libraries for processing video, audio, and image data, and applies machine learning models to achieve various functionalities such as pose estimation, gesture recognition, and face detection. MediaPipe is designed to offer efficient, flexible, and easy-to-use solutions, enabling developers to quickly build a variety of multimedia processing applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb510e4a-d332-4c27-81f6-59c7c85cd170",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Preparation\n",
    "\n",
    "Since the product automatically runs the main program at startup, which occupies the camera resource, this tutorial cannot be used in such situations. You need to terminate the main program or disable its automatic startup before restarting the robot.\n",
    "\n",
    "It's worth noting that because the robot's main program uses multi-threading and is configured to run automatically at startup through crontab, the usual method sudo killall python typically doesn't work. Therefore, we'll introduce the method of disabling the automatic startup of the main program here.\n",
    "\n",
    "### Terminate the Main Program\n",
    "\n",
    "1. Click the \"+\" icon next to the tab for this page to open a new tab called \"Launcher.\"\n",
    "2. Click on \"Terminal\" under \"Other\" to open a terminal window.\n",
    "3. Type bash into the terminal window and press Enter.\n",
    "4. Now you can use the Bash Shell to control the robot.\n",
    "5. Enter the command: crontab -e.\n",
    "6. If prompted to choose an editor, enter 1 and press Enter to select nano.\n",
    "7. After opening the crontab configuration file, you'll see the following two lines:\n",
    "> @reboot ~/ugv_pt_rpi/ugv-env/bin/python ~/ugv_pt_rpi/app.py >> ~/ugv.log 2>&1\n",
    ">\n",
    ">@reboot /bin/bash ~/ugv_pt_rpi/start_jupyter.sh >> ~/jupyter_log.log 2>&1\n",
    "\n",
    "8. Add a # character at the beginning of the line with ……app.py >> …… to comment out this line.\n",
    "> #@reboot ~/ugv_pt_rpi/ugv-env/bin/python ~/ugv_pt_rpi/app.py >> ~/ugv.log 2>&1\n",
    ">\n",
    ">@reboot /bin/bash ~/ugv_pt_rpi/start_jupyter.sh >> ~/jupyter_log.log 2>&1\n",
    "\n",
    "9. Press Ctrl + X in the terminal window to exit. It will ask you Save modified buffer? Enter Y and press Enter to save the changes.\n",
    "10. Reboot the device. Note that this process will temporarily close the current Jupyter Lab session. If you didn't comment out ……start_jupyter.sh >>…… in the previous step, you can still use Jupyter Lab normally after the robot reboots (JupyterLab and the robot's main program app.py run independently). You may need to refresh the page.\n",
    "11. One thing to note is that since the lower machine continues to communicate with the upper machine through the serial port, the upper machine may not start up properly during the restart process due to the continuous change of serial port levels. Taking the case where the upper machine is a Raspberry Pi, after the Raspberry Pi is shut down and the green LED is constantly on without the green LED blinking, you can turn off the power switch of the robot, then turn it on again, and the robot will restart normally.\n",
    "12. Enter the reboot command: sudo reboot.\n",
    "13. After waiting for the device to restart (during the restart process, the green LED of the Raspberry Pi will blink, and when the frequency of the green LED blinking decreases or goes out, it means that the startup is successful), refresh the page and continue with the remaining part of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c9488-47b8-4bec-8e2b-ccba2486f55d",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "The following code block can be run directly:\n",
    "\n",
    "1. Select the code block below.\n",
    "2. Press Shift + Enter to run the code block.\n",
    "3. Watch the real-time video window.\n",
    "4. Press STOP to close the real-time video and release the camera resources.\n",
    "\n",
    "### If you cannot see the real-time camera feed when running:\n",
    "\n",
    "- Click on Kernel -> Shut down all kernels above.\n",
    "- Close the current section tab and open it again.\n",
    "- Click `STOP` to release the camera resources, then run the code block again.\n",
    "- Reboot the device.\n",
    "\n",
    "### Features of this Section\n",
    "\n",
    "When the code block runs successfully, you can place your hand in front of the camera, and the real-time video frame will display annotations indicating the joints of the hand. These annotations will change with the movement of your hand, and the positions of each joint will be outputted as well, facilitating further development for gesture control.\r\n",
    "\r\n",
    "MediaPipe's gesture recognition process uses different names to correspond to different joints. You can retrieve the position information of a joint by calling its corresponding number.\n",
    "\n",
    "#### MediaPipe Han\n",
    "d\n",
    "1.WRIST\n",
    "\n",
    "2.THUMB_CMC\n",
    "\n",
    "3.THUMB_MCP\n",
    "\n",
    "4.THUMB_IP\n",
    "\n",
    "5.THUMB_TIP\n",
    "\n",
    "6.INDEX_FINGER_MCP\n",
    "\n",
    "7.INDEX_FINGER_PIP\n",
    "\n",
    "8.INDEX_FINGER_DIP\n",
    "\n",
    "9.INDEX_FINGER_TIP\n",
    "\n",
    "10.MIDDLE_FINGER_MCP\n",
    "\n",
    "11.MIDDLE_FINGER_PIP\n",
    "\n",
    "12.MIDDLE_FINGER_DIP\n",
    "\n",
    "13.MIDDLE_FINGER_TIP\n",
    "\n",
    "14.RING_FINGER_MCP\n",
    "\n",
    "15.RING_FINGER_PIP\n",
    "\n",
    "16.RING_FINGER_DIP\n",
    "\n",
    "17.RING_FINGER_TIP\n",
    "\n",
    "18.PINKY_MCP\n",
    "\n",
    "19.PINKY_PIP\n",
    "\n",
    "20.PINKY_DIP\n",
    "\n",
    "21.PINKY_TIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30eb25-3807-4b5c-9702-13f393c32fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imutils, math\n",
    "from picamera2 import Picamera2  # 用于访问 Raspberry Pi Camera 的库\n",
    "from IPython.display import display, Image  # 用于在 Jupyter Notebook 中显示图像\n",
    "import ipywidgets as widgets  # 用于创建交互式界面的小部件，如按钮\n",
    "import threading  # 用于创建新线程，以便异步执行任务\n",
    "import mediapipe as mp  # 导入 MediaPipe 库，用于手部关键点检测\n",
    "\n",
    "\n",
    "# 创建一个“停止”按钮，用户可以通过点击它来停止视频流\n",
    "# ================\n",
    "stopButton = widgets.ToggleButton(\n",
    "    value=False,\n",
    "    description='Stop',\n",
    "    disabled=False,\n",
    "    button_style='danger', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Description',\n",
    "    icon='square' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "# 初始化 MediaPipe 绘图工具和手部关键点检测模型\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "mpHands = mp.solutions.hands\n",
    "hands = mpHands.Hands(max_num_hands=1) # 初始化手部关键点检测模型，最多检测一只手\n",
    "\n",
    "# 定义显示函数，用于处理视频帧并进行手部关键点检测\n",
    "def view(button):\n",
    "    # picam2 = Picamera2()  # 创建 Picamera2 的实例\n",
    "    # picam2.configure(picam2.create_video_configuration(main={\"format\": 'XRGB8888', \"size\": (640, 480)}))  # 配置摄像头参数\n",
    "    # picam2.start()  # 启动摄像头\n",
    "    \n",
    "    camera = cv2.VideoCapture(-1) \n",
    "    camera.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "    camera.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    \n",
    "    display_handle=display(None, display_id=True)  # 创建显示句柄用于更新显示的图像\n",
    "    \n",
    "    while True:\n",
    "        # frame = picam2.capture_array()\n",
    "        _, frame = camera.read()\n",
    "        # frame = cv2.flip(frame, 1) # if your camera reverses your image\n",
    "\n",
    "        # frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        results = hands.process(img)\n",
    "\n",
    "        # 如果检测到手部关键\n",
    "        if results.multi_hand_landmarks:\n",
    "            for handLms in results.multi_hand_landmarks: # 遍历检测到的每只手\n",
    "                # 绘制手部关键点\n",
    "                for id, lm in enumerate(handLms.landmark):\n",
    "                    h, w, c = img.shape\n",
    "                    cx, cy = int(lm.x * w), int(lm.y * h)  # 计算关键点在图像中的位置\n",
    "                    cv2.circle(img, (cx, cy), 5, (255, 0, 0), -1)  # 在关键点位置绘制圆点\n",
    "\n",
    "                \n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                mpDraw.draw_landmarks(frame, handLms, mpHands.HAND_CONNECTIONS) # 绘制手部骨架连接线\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "                target_pos = handLms.landmark[mpHands.HandLandmark.INDEX_FINGER_TIP]\n",
    "\n",
    "        _, frame = cv2.imencode('.jpeg', frame)\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "        if stopButton.value==True:\n",
    "            picam2.close()\n",
    "            display_handle.update(None)\n",
    "\n",
    "# 显示“停止”按钮并启动显示函数的线程\n",
    "# ================\n",
    "display(stopButton)\n",
    "thread = threading.Thread(target=view, args=(stopButton,))\n",
    "thread.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
